# Fuka-6.0
First Universal Kommon Ancestor - by Yasas (‡∂∫‡∑É‡∑É‡∑ä ‡∂¥‡∑ú‡∂±‡∑ä‡∑Ä‡∑ì‡∂ª)


Fuka-6.0 ‚Äî Emergent Computation on Capacitor Substrates

Fuka-6.0 is a physics-first simulation platform exploring how computation, symbolic code, and adaptive phenotype-like behavior can emerge from a primitive network of capacitors interacting with an environment.

There are:
	‚Ä¢	No neurons
	‚Ä¢	No logic gates
	‚Ä¢	No symbolic rules
	‚Ä¢	No backpropagation
	‚Ä¢	No pre-programmed intelligence

Only:
	‚Ä¢	capacitor dynamics
	‚Ä¢	local plasticity
	‚Ä¢	energy sources
	‚Ä¢	environmental waves
	‚Ä¢	self-organization

From this, the system develops:
	‚Ä¢	discrete attractor states (symbols)
	‚Ä¢	sequences of attractors (code)
	‚Ä¢	evolving connection topology (hardware)
	‚Ä¢	adaptive long-memory pockets
	‚Ä¢	environment-modifying behavior (phenotype)

This is top-down and bottom-up model of how computation can arise from physical substrates.

‚∏ª

1. Motivation

Biological computation is not designed.
It emerges from:
	1.	physical substrates
	2.	energy gradients
	3.	self-reinforcing attractors
	4.	code-like symbolic transitions
	5.	phenotype behavior that acts back on the environment

Fuka-6.0 aims to show the earliest version of this process using the simplest possible physical substrate that can compute:
a network of capacitors with leak, coupling, and local adaptation.

The goal is to study how:
	‚Ä¢	computation
	‚Ä¢	memory
	‚Ä¢	code
	‚Ä¢	hardware
	‚Ä¢	and phenotype

can emerge together from pure physics.

‚∏ª

2. Capacitor Substrate Model

The substrate consists of N capacitors.
Each capacitor i has voltage V_i(t) and capacitance C_i.

Dynamics follow:

C_i \frac{dV_i}{dt}
= -\lambda_i V_i
+ \sum_j g_{ij}(V_j - V_i)
+ I_i(t)

Where:
	‚Ä¢	C_i ‚Äî capacitance
	‚Ä¢	\lambda_i ‚Äî leakage (natural memory decay)
	‚Ä¢	g_{ij} ‚Äî conductance between capacitor i and j
	‚Ä¢	I_i(t) ‚Äî environmental energy injected into capacitor i

This is the minimal physical substrate able to store and transform information.

‚∏ª

3. Environment

The environment provides fluctuating energy input:

I_i(t) = f_i(E(t), x_i)

Where:
	‚Ä¢	E(t) ‚Äî global environmental state
	‚Ä¢	x_i ‚Äî spatial or structural position
	‚Ä¢	f_i ‚Äî mapping from environment to excitation

The environment is purely physical, not symbolic.

‚∏ª

4. Plasticity / Learning Rule

The substrate adapts using a purely local learning rule that strengthens useful connections and weakens useless ones.

\frac{dg_{ij}}{dt}
= \eta F(t)\left( V_i V_j - \alpha g_{ij} \right)

Where:
	‚Ä¢	\eta ‚Äî learning rate
	‚Ä¢	\alpha ‚Äî decay
	‚Ä¢	F(t) ‚Äî global stability pressure

4.1 Stability Pressure

F(t) = -\frac{1}{N}\sum_i \left(\frac{dV_i}{dt}\right)^2

Interpretation:
	‚Ä¢	low turbulence ‚Üí high F(t) ‚Üí reinforce connections
	‚Ä¢	high turbulence ‚Üí low F(t) ‚Üí connections decay

This forms the basis of emergent ‚Äúevolution.‚Äù

‚∏ª

5. Attractors ‚Äî The First Symbols

When the environment repeatedly injects energy, the substrate settles into stable states:

\mathbf{V}(t) \rightarrow A_k

Each attractor A_k is:
	‚Ä¢	reproducible
	‚Ä¢	stable under small perturbations
	‚Ä¢	low turbulence
	‚Ä¢	persistent

These attractors form the first alphabet of the system.

They are the proto-symbols.

‚∏ª

6. Attractor Sequences ‚Äî The First Code

Environmental waves arrive in discrete ‚Äúslots‚Äù:
	‚Ä¢	energy pulse
	‚Ä¢	relaxation
	‚Ä¢	stabilize into attractor

Sampling the attractor after each slot produces:

A_{k_1}, A_{k_2}, A_{k_3}, \dots

This is the proto-code.

It is not designed.
It emerges from substrate physics.

‚∏ª

7. Transition Graph ‚Äî The Proto Grammar

Transitions between attractors:

A_i \rightarrow A_j

form a directed graph.

Repeated transitions form:
	‚Ä¢	syntax
	‚Ä¢	rules
	‚Ä¢	operators
	‚Ä¢	compositional functions
	‚Ä¢	memory cycles
	‚Ä¢	branching structures

The transition graph is the early form of:
	‚Ä¢	grammar
	‚Ä¢	program
	‚Ä¢	computation

‚∏ª

8. Emergent Hardware

The substrate gradually organizes into:
	‚Ä¢	hubs
	‚Ä¢	oscillators
	‚Ä¢	gating motifs
	‚Ä¢	long-range pathways
	‚Ä¢	slow-drift memory pockets
	‚Ä¢	feedback loops

This evolving topology is the hardware.

There is no separate ‚Äúchip.‚Äù
Hardware is whatever physical structure repeatedly stabilizes under environmental pressure.

‚∏ª

9. Phenotype: Acting Back on the Environment

The ultimate milestone is when the substrate:
	1.	performs computation
	2.	creates stable behavior
	3.	modifies its environment
	4.	which then affects its own future states

This forms a closed evolutionary cycle:

\text{substrate} \;\leftrightarrow\; \text{code} \;\leftrightarrow\; \text{environment}

This is the minimal definition of a phenotype in this framework.

‚∏ª

10. Toward Universal Computation

The long-term objective is to show that Fuka-6.0 naturally evolves:
	1.	finite attractor alphabet
	2.	stable attractor sequences
	3.	compositional transition grammar
	4.	persistent multi-slot memory
	5.	gated read/write structures
	6.	branching transitions
	7.	feedback loops that represent functions

This combination yields the primitive conditions of a Turing-complete system emerging from physics alone.

‚∏ª

11. Roadmap

üìò How Capacitors Work ‚Äî and How Fuka Capacitors Compute

This section explains, in simple language, how real capacitors behave and how the Fuka-6.0 substrate uses a generalized capacitor model to create emergent symbols, code, and hardware.

### How a Real Capacitor Works
![Capacitor Physics Explained](images/3D3711EE-FB1C-4AEE-9352-DF266EB53D5C.png)


‚∏ª

üß© 1. What is a capacitor?

A capacitor is the simplest device that can store and change electrical state.


It holds energy by separating charge.
Three important facts:

‚úî It has a voltage

‚úî It changes that voltage over time

‚úî It stores energy in the electric field

The equations are:

Q = C V                  (charge = capacitance √ó voltage)
I = C dV/dt              (current changes voltage)
E = ¬Ω C V¬≤               (energy stored)


‚∏ª

üß© 2. Why capacitors matter for computation

Capacitors naturally create:
	‚Ä¢	memory (stored voltage)
	‚Ä¢	dynamics (voltages evolve in time)
	‚Ä¢	attractors (stable voltage patterns)
	‚Ä¢	pattern separation (different states converge to different minima)

These are the same ingredients used by:
	‚Ä¢	neural networks
	‚Ä¢	analog computers
	‚Ä¢	Hopfield networks
	‚Ä¢	early biological systems

Capacitor networks naturally form state machines.

### How Fuka-6.0 Capacitors Work
![Fuka Capacitor Network](images/96FB08D3-A8E0-4225-9267-3B54A23906A5.png)

‚∏ª

üß© 3. The Fuka-6.0 idea: A universe of capacitors

In Fuka-6.0, we generalize this idea.

We simulate a network of n abstract capacitors:

x = [x‚ÇÅ, x‚ÇÇ, x‚ÇÉ, ..., x‚Çô]

Each value x·µ¢ is the voltage of that capacitor at time t.

These capacitors interact through a conductance matrix:

g[i,j] = strength of coupling from capacitor j ‚Üí i

This determines how charge ‚Äúflows‚Äù between units.

### Transition Graph (Attractor Finite-State Machine)
![Transition Graph](images/ACEA2CE6-E90C-4611-8EC3-1918D595E02F.png)


‚∏ª

üß© 4. What drives the capacitors?

There are three forces that change capacitor voltages.


‚∏ª

(1) Internal dynamics (like charge flow)

Capacitors equalize through conductances:

Œîx·µ¢ ‚àù Œ£ g·µ¢‚±º ( x‚±º ‚àí x·µ¢ )

This creates:
	‚Ä¢	attractors
	‚Ä¢	stable patterns
	‚Ä¢	state convergence

These attractors eventually become symbols.

‚∏ª

(2) External environment forcing

The environment (A, B, C or analog wave) pushes the system:

Œîx·µ¢ ‚àù Œ± ¬∑ E(t)

This is like an electrode injecting charge.

Environment ‚Üí shapes the attractor basins ‚Üí creates a consistent alphabet.

‚∏ª

(3) Plasticity (rewiring the hardware)

Conductances change over time:
	‚Ä¢	connections strengthen
	‚Ä¢	unused paths decay
	‚Ä¢	modules form

This is how the substrate self-builds its own hardware.

In code, g is updated by local rules:

g‚Çú‚Çä‚ÇÅ = g‚Çú + f(local_state)

This is the heart of Fuka‚Äôs emergent hardware.

‚∏ª

üß© 5. What encodes a symbol?

A symbol is not stored explicitly.

Instead:

‚úî Symbols = attractor basins in state space

Example:

A = cluster of states near pattern pA
B = cluster near pB
C = cluster near pC

The substrate repeatedly falls into these patterns whenever the environment returns to the same regime.

This is how the alphabet emerges.

‚∏ª

üß© 6. What encodes code?

Code emerges as the sequence of transitions between attractors.

Example:

A ‚Üí B ‚Üí B ‚Üí A ‚Üí ...

Each arrow is a directed transition in the attractor graph.

This graph is physically created by:
	‚Ä¢	the capacitor dynamics
	‚Ä¢	the conductance layout
	‚Ä¢	the influence of environment

This is equivalent to a proto grammar or a finite state machine.

‚∏ª

üß© 7. What encodes hardware?

Hardware = the conductance matrix g.

This is the ‚Äúwiring‚Äù of the substrate:

g =
[ g11 g12 g13 ... ]
[ g21 g22 g23 ... ]
[ ...            ]

Over time:
	‚Ä¢	g acquires structure
	‚Ä¢	modules appear
	‚Ä¢	repeated motifs emerge
	‚Ä¢	certain pathways become specialized

The substrate is literally building its own circuitry.

This is the link between:

physics ‚Üí hardware ‚Üí symbols ‚Üí code ‚Üí adaptation

‚∏ª

üß© 8. Full mapping between physics and simulation

Real World	Fuka Capacitor Model	Meaning
Voltage	x·µ¢	State/memory
Charge flow	Œ£ g·µ¢‚±º(x‚±º ‚àí x·µ¢)	Interaction dynamics
External field	E(t)	Environment force
Cap geometry	plasticity	Hardware evolution
Energy minima	attractors	Symbols
State transitions	attractor shifts	Code
Circuit topology	conductance g	Hardware

This is the clean unification:

Capacitors ‚Üí attractors ‚Üí symbols ‚Üí code ‚Üí hardware ‚Üí adaptation


‚∏ª

üß© 9. Why this is important

This framework explains how:
	‚Ä¢	computation can emerge from physics
	‚Ä¢	symbols can emerge from pure dynamics
	‚Ä¢	hardware and code co-evolve
	‚Ä¢	adaptation becomes possible without pre-built structures
	‚Ä¢	biological systems may have originated

This is the conceptual foundation of Fuka-6.0.

‚∏ª

Phase-6 Phenotype Results

Fuka-6.0 ‚Äì Emergent alphabet, grammar, and substrate fingerprints

This section summarizes the Phase-6.1 phenotype run:

runs/exp_phenotype_fixed_20251129_230800.npz

From a single long run, the system produced a finite attractor alphabet, a probabilistic grammar over that alphabet, and stable physical ‚Äúfingerprints‚Äù for each attractor. All of this emerges from local capacitor dynamics and plasticity, without any external supervision.

‚∏ª

1. Core attractor alphabet

From 680 attractor samples, cosine clustering produced 293 distinct clusters.
If we keep only clusters that appear at least 10 times, we get a core alphabet of 9 attractors:

T1, T2, T3, T5, T8, T12, T14, T17, T24

Key statistics:
	‚Ä¢	Core alphabet coverage (clusters with count ‚â• 10): 57.9% of all samples.
	‚Ä¢	The largest clusters have sizes: 95, 69, 61, 41, 33, 32, 32, 16, 15.
	‚Ä¢	The top 4 attractors alone cover roughly 40‚Äì45% of all samples.

This means the substrate settles repeatedly into a small, re-used set of attractor codes, rather than visiting all states uniformly. In the Fuka view, this set is the emergent symbolic alphabet of the phenotype.

‚∏ª

2. Emergent phenotype grammar

We can build a Markov grammar over the core alphabet by looking at transitions from one core attractor to the next. The strongest core-to-core transitions are:
	‚Ä¢	T12 ‚Üí T24 with probability ~0.73 (30 transitions)
	‚Ä¢	T1 ‚Üí T17 with probability ~0.52 (32 transitions)
	‚Ä¢	T2 ‚Üí T3 with probability ~0.45 (31 transitions)
	‚Ä¢	Self-loops such as T2 ‚Üí T2 and T1 ‚Üí T1
	‚Ä¢	Connector edges like T2 ‚Üí T8, T3 ‚Üí T5, T24 ‚Üí T5

This defines a directed grammar where some attractors tend to follow others in a strongly biased way. The result is not a random walk. It is closer to a simple symbolic language with:
	‚Ä¢	Recurrent motifs (T12‚ÜîT24, T1‚ÜíT17‚ÜíT5, T2‚ÜíT3‚ÜíT5)
	‚Ä¢	Stable loops and hubs
	‚Ä¢	A small set of high-probability ‚Äúgrammar rules‚Äù

![Phenotype grammar](images/phenotype_grammar_latest.png)

Figure 1 ‚Äì Core phenotype grammar graph

Figure 1: Graph of the core attractor alphabet. Nodes are the 9 most frequent attractors (T1, T2, T3, T5, T8, T12, T14, T17, T24). Directed edges show transitions between them. Edge thickness and labels encode transition probability and counts. The strongest motifs are T12 ‚Üí T24, T1 ‚Üí T17, and T2 ‚Üí T3, indicating that the phenotype dynamics concentrate onto a small number of repeated symbolic patterns.

‚∏ª

3. Environment-dependent syntax

The environment has a scalar state E(t) which is updated by the substrate and in turn modulates how strongly energy is injected. If we sample the environment value at the same times we take attractor snapshots and split by the median of E:
	‚Ä¢	Low-E band: more exploratory, almost no stable core-to-core transitions.
	‚Ä¢	High-E band: grammar becomes much more deterministic:
	‚Ä¢	T1 ‚Üí T17 reaches probability 1.0 in high-E samples.
	‚Ä¢	T12 ‚Üí T24 reaches probability ~0.94.
	‚Ä¢	T2 ‚Üí T3 reaches probability ~0.63.

So when the environment is ‚Äúenergized‚Äù, the system collapses into a more rigid grammar. When energy is lower, it wanders and explores more codes.

In other words:

The scalar environment state controls how grammatical the behaviour is.
High energy = more strongly structured syntax.
Low energy = more diffuse, exploratory syntax.

‚∏ª

4. Attractor fingerprints in substrate space

Each attractor can also be represented by its physical fingerprint: the mean voltage vector of the capacitor network whenever the system visits that attractor.

By projecting these fingerprints to 2D using PCA, we see that:
	‚Ä¢	Attractors that are grammatically connected (for example T12 and T24, or T1 and T17) tend to lie close to each other in fingerprint space.
	‚Ä¢	Attractors that rarely or never follow each other are more separated.

This means the symbolic grammar is not arbitrary. It is grounded in the actual physical geometry of the capacitor voltages.

![Attractor fingerprints](images/phenotype_fingerprints_latest.png)

Figure 2 ‚Äì PCA map of core attractor fingerprints

Figure 2: PCA projection of the mean voltage fingerprints for the 9 core attractors. Each point is one attractor (T1, T2, T3, T5, T8, T12, T14, T17, T24). Attractors that often follow each other in the grammar tend to be neighbours in this space (for example T12 and T24). This links symbolic transitions directly to physical similarity in the capacitor substrate.

‚∏ª

5. Interpretation

Putting everything together:
	‚Ä¢	A finite attractor alphabet emerges spontaneously from local physics.
	‚Ä¢	This alphabet carries a probabilistic grammar with strong repeated motifs.
	‚Ä¢	The grammar sharpens at high environment energy and weakens at low energy.
	‚Ä¢	Attractor identities are not abstract variables; they are bound to physical fingerprints in the capacitor network.

From the Fuka-6.0 point of view, this run is a first concrete example of:

A physical substrate that invents its own symbols, grammar, and phenotype-level behaviour from nothing more than energy flow, local plasticity, and a closed loop with its environment.


